{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Normalize Reviewer Scores\n",
    "## Also known as \"Why can't reviewers use the middle of the range as average?!?!\"\n",
    "\n",
    "Herein I outline how to perform z-score-based normalization of merit reviewer scores using a custom Python code module called ScoreNormalizer. Hopefully, if you're reading this iPython notebook in its native environment, you'll be able to run all of the code discussed herein. I'll try to provide guidance on changes based upon your directory structure, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Things First\n",
    "\n",
    "Data wrangling. The code used here all assumes a uniform format to the input CSV file of reviewer scores. This was based off of the 2017 version of the EERE eXCHANGE system's Review Details Grid. It's possible that the file structure of this report has changed since 2017 (and/or that eXCHANGE is no longer a thing at all!), so please check the documentation inside of the ScoreNormalizer code to find out what input file format it needs.\n",
    "\n",
    "Ultimately, you're in good shape if the input file has the following columns included (with each individual reviewer's single-criterion-score, such as for Technical Merit, as individual rows in the file):\n",
    "\n",
    "1. **Review Status:** this should only have values \"Not Started\", \"Review Started\", or \"Complete\"\n",
    "2. **Submission Type:** only values expected here currently are \"Concept Paper\" or \"Full Application\"\n",
    "2. **Reviewer Full Name** \n",
    "3. **Control Number**\n",
    "4. **Numeric Rating:** this is just a reviewer's raw score for a given review criterion in a given project\n",
    "5. **Criteria Weight:** it's expected that this is an integer value representing a percentage (e.g. 40 = 40% criterion weighting) and thus the code will automatically divide this by 100 prior to performing any calculations with it.\n",
    "\n",
    "## MAKE SURE YOU SAVE THE FILE AS CSV UTF-8 (COMMA-DELIMITED) (.CSV)\n",
    "This is necessary to make sure Python doesn't incorrectly interpret the encoding of the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do this!\n",
    "\n",
    "Once we have our input file in order, we can commence analysis. The steps coded below go as follows:\n",
    "\n",
    "1. Setup our system path to see the code module folder. We assume here that our input data files are all located in this iPython notebook's working directory and that the code modules are .PY files located in a folder within the working directory called 'Normalizer'.\n",
    "2. Import the data file and display the head of the DataFrame that is constructed using that file.\n",
    "3. Process the input DataFrame using reviewer-specific z-scores and putting those z-scores on to a common distribution based off of the allowed range of scores defined by the FOA.\n",
    "4. Export the normalized scores as a CSV file with the same name as the input file but with `_NORMALIZED` added to the end.\n",
    "5. After the scores are exported as a CSV file, there's also some fun quick comparisons you can do to see what the highest-end reviews were pre- and post-normalization, as well as look at what reviews ended up getting a score boost as a result of this process.\n",
    "    * NOTE: we have observed thus far that scores tend to see a suppression effect from this normalization approach (reflecting a general positive bias for the average reviewer). That's why this last comparison is likely to return few, if any, results and is thus interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('Normalizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ScoreNormalizer as sc\n",
    "\n",
    "input_filename = 'DE-FOA-0001840 Review Details - Concept Papers.csv'\n",
    "\n",
    "df = sc.import_data(input_filename)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Make sure you change the `score_range` variable in the next code block to reflect the minimum and maximum possible scores for the applications you are reviewing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#score_range = (minimum_score, max_score)\n",
    "\n",
    "output = sc.normalize_scores(df, score_range = (-1,1))\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = input_filename[:-4] + \"_NORMALIZED.xlsx\"\n",
    "sc.export_data(output, output_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ScoreNormalizing]",
   "language": "python",
   "name": "conda-env-ScoreNormalizing-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
